name: PyTest PySpark constructor

on:
  pull_request:
    types: [opened, labeled, synchronize]  # Triggers on PR creation, updates, and label changes

env:
  PY_COLORS: 1

jobs:

  pytest-pyspark-constructor:
    if: ${{ contains(github.event.pull_request.labels.*.name, 'pyspark') || contains(github.event.pull_request.labels.*.name, 'spark-like') || contains(github.event.pull_request.labels.*.name, 'release')}}
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: "true"
          cache-suffix: ${{ matrix.python-version }}
          cache-dependency-glob: "pyproject.toml"
      - name: install-reqs
        run: uv pip install -e . --group core-tests --group extra --system
      - name: install pyspark
        run: uv pip install -e ".[pyspark]" --system
      - name: show-deps
        run: uv pip freeze
      - name: Run pytest
        run: pytest tests --cov=narwhals/_spark_like --cov-fail-under=95 --runslow --constructors pyspark


  pytest-pyspark-connect-constructor:
    if: ${{ contains(github.event.pull_request.labels.*.name, 'pyspark-connect') || contains(github.event.pull_request.labels.*.name, 'release') }}
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
        os: [ubuntu-latest]
    env:
      SPARK_VERSION: 3.5.5
      SPARK_PORT: 15002
      SPARK_CONNECT: true
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: "true"
          cache-suffix: ${{ matrix.python-version }}
          cache-dependency-glob: "pyproject.toml"

      - name: Install Java 17
        uses: actions/setup-java@v4
        with:
          distribution: zulu
          java-version: 17

      - name: install-reqs
        run: uv pip install -e . --group core-tests --group extra --system
      - name: install pyspark
        run: echo "setuptools<78" | uv pip install -e . "pyspark[connect]==${SPARK_VERSION}" --system
      - name: show-deps
        run: uv pip freeze
      
      - name: Cache Spark
        id: cache-spark
        uses: actions/cache@v4
        with:
          path: /opt/spark
          key: spark-${{ env.SPARK_VERSION }}-bin-hadoop3

      - name: Download Spark
        if: steps.cache-spark.outputs.cache-hit != 'true'
        run: |
          wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz
          tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz
          sudo mv spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark
      
      - name: Set Spark env variables
        run: |
          echo "SPARK_HOME=/opt/spark" >> $GITHUB_ENV
          echo "/opt/spark/bin" >> $GITHUB_PATH

      - name: Start Spark Connect server
        run: |
          $SPARK_HOME/sbin/start-connect-server.sh \
            --packages org.apache.spark:spark-connect_2.12:${SPARK_VERSION} \
            --conf spark.connect.grpc.binding.port=${SPARK_PORT}
          sleep 5
          echo "Spark Connect server started"

      - name: Run pytest
        run: pytest tests --cov=narwhals/_spark_like --cov-fail-under=95 --runslow --constructors "pyspark[connect]"

      - name: Stop Spark Connect server
        if: always()
        run: $SPARK_HOME/sbin/stop-connect-server.sh
